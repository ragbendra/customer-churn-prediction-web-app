{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Objective**: Statistical analysis and visualization of churn patterns\n",
    "\n",
    "**Outputs**:\n",
    "- Distribution plots by churn class\n",
    "- Statistical tests (T-tests, Chi-square)\n",
    "- Correlation heatmap with multicollinearity detection\n",
    "- Top churn signal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Libraries loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "RAW_DATA_PATH = Path('../data/01_raw')\n",
    "REPORTING_PATH = Path('../data/08_reporting')\n",
    "REPORTING_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(RAW_DATA_PATH / 'cell2celltrain.csv')\n",
    "print(f\"Loaded: {df.shape[0]:,} rows × {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Univariate Analysis - Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key numerical features for analysis\n",
    "key_numerical = [\n",
    "    'MonthlyRevenue', 'MonthlyMinutes', 'MonthsInService', \n",
    "    'TotalRecurringCharge', 'DroppedCalls', 'BlockedCalls',\n",
    "    'CustomerCareCalls', 'OverageMinutes'\n",
    "]\n",
    "key_numerical = [f for f in key_numerical if f in df.columns]\n",
    "\n",
    "target = 'Churn'\n",
    "df_churn = df[df[target] == 1]\n",
    "df_retain = df[df[target] == 0]\n",
    "\n",
    "print(f\"Churners: {len(df_churn):,}\")\n",
    "print(f\"Retained: {len(df_retain):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots overlaid by churn\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(key_numerical[:8]):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot distributions\n",
    "    sns.kdeplot(data=df_retain[col].dropna(), ax=ax, label='Retained', color='#2ecc71', fill=True, alpha=0.3)\n",
    "    sns.kdeplot(data=df_churn[col].dropna(), ax=ax, label='Churned', color='#e74c3c', fill=True, alpha=0.3)\n",
    "    \n",
    "    ax.set_title(col, fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTING_PATH / 'distribution_by_churn.png', dpi=150)\n",
    "plt.show()\n",
    "print(f\" Saved: distribution_by_churn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(key_numerical[:8]):\n",
    "    ax = axes[i]\n",
    "    df.boxplot(column=col, by=target, ax=ax)\n",
    "    ax.set_title(col, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Churn')\n",
    "\n",
    "plt.suptitle('Box Plots by Churn Status', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTING_PATH / 'boxplots_by_churn.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Tests (T-Tests for Numerical Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ttest(df, feature, target='Churn'):\n",
    "    \"\"\"Perform independent T-test between churn groups.\"\"\"\n",
    "    churners = df[df[target] == 1][feature].dropna()\n",
    "    retained = df[df[target] == 0][feature].dropna()\n",
    "    \n",
    "    statistic, pvalue = stats.ttest_ind(churners, retained, equal_var=False)\n",
    "    \n",
    "    return {\n",
    "        'Feature': feature,\n",
    "        'Mean (Churn)': churners.mean(),\n",
    "        'Mean (Retain)': retained.mean(),\n",
    "        'Difference': churners.mean() - retained.mean(),\n",
    "        'T-Statistic': statistic,\n",
    "        'P-Value': pvalue,\n",
    "        'Significant': ' Yes' if pvalue < 0.05 else ' No'\n",
    "    }\n",
    "\n",
    "# Perform T-tests on all numerical features\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "numerical_cols = [c for c in numerical_cols if c != target and 'ID' not in c.upper()]\n",
    "\n",
    "ttest_results = [perform_ttest(df, col) for col in numerical_cols]\n",
    "ttest_df = pd.DataFrame(ttest_results)\n",
    "ttest_df = ttest_df.sort_values('P-Value')\n",
    "\n",
    "print(\" T-TEST RESULTS (Top 20 by Significance)\")\n",
    "print(\"=\"*80)\n",
    "display(ttest_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top significant features\n",
    "significant_features = ttest_df[ttest_df['P-Value'] < 0.05]['Feature'].tolist()\n",
    "print(f\"\\n Significant features (p < 0.05): {len(significant_features)}\")\n",
    "print(significant_features[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Churn Rate by Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features to analyze\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"Categorical columns: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_churn_rate_by_category(df, feature, target='Churn'):\n",
    "    \"\"\"Calculate churn rate for each category.\"\"\"\n",
    "    churn_rate = df.groupby(feature)[target].agg(['mean', 'count'])\n",
    "    churn_rate.columns = ['Churn Rate', 'Count']\n",
    "    churn_rate['Churn Rate'] = churn_rate['Churn Rate'] * 100\n",
    "    return churn_rate.sort_values('Churn Rate', ascending=False)\n",
    "\n",
    "# Analyze key categorical features\n",
    "key_categorical = ['MaritalStatus', 'Homeownership', 'IncomeGroup', 'CreditRating']\n",
    "key_categorical = [c for c in key_categorical if c in df.columns]\n",
    "\n",
    "for col in key_categorical:\n",
    "    print(f\"\\n Churn Rate by {col}:\")\n",
    "    display(calculate_churn_rate_by_category(df, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize churn rate by key categories\n",
    "if len(key_categorical) > 0:\n",
    "    fig, axes = plt.subplots(1, min(4, len(key_categorical)), figsize=(16, 5))\n",
    "    if len(key_categorical) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, col in enumerate(key_categorical[:4]):\n",
    "        ax = axes[i]\n",
    "        churn_by_cat = df.groupby(col)[target].mean() * 100\n",
    "        churn_by_cat.plot(kind='bar', ax=ax, color='#3498db', edgecolor='black')\n",
    "        ax.set_title(f'Churn Rate by {col}', fontweight='bold')\n",
    "        ax.set_ylabel('Churn Rate (%)')\n",
    "        ax.axhline(y=df[target].mean()*100, color='red', linestyle='--', label='Overall Rate')\n",
    "        ax.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTING_PATH / 'churn_by_category.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chi-Square Tests for Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_chi_square(df, feature, target='Churn'):\n",
    "    \"\"\"Perform Chi-square test for independence.\"\"\"\n",
    "    contingency = pd.crosstab(df[feature], df[target])\n",
    "    chi2, pvalue, dof, expected = stats.chi2_contingency(contingency)\n",
    "    \n",
    "    # Cramér's V for effect size\n",
    "    n = contingency.sum().sum()\n",
    "    cramers_v = np.sqrt(chi2 / (n * (min(contingency.shape) - 1)))\n",
    "    \n",
    "    return {\n",
    "        'Feature': feature,\n",
    "        'Chi-Square': chi2,\n",
    "        'P-Value': pvalue,\n",
    "        'Cramér\\'s V': cramers_v,\n",
    "        'Significant': ' Yes' if pvalue < 0.05 else ' No'\n",
    "    }\n",
    "\n",
    "# Perform chi-square tests\n",
    "chi_results = []\n",
    "for col in categorical_cols:\n",
    "    try:\n",
    "        result = perform_chi_square(df, col)\n",
    "        chi_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {col}: {e}\")\n",
    "\n",
    "chi_df = pd.DataFrame(chi_results).sort_values('P-Value')\n",
    "\n",
    "print(\" CHI-SQUARE TEST RESULTS\")\n",
    "print(\"=\"*80)\n",
    "display(chi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for numerical features\n",
    "numerical_df = df.select_dtypes(include=['int64', 'float64'])\n",
    "corr_matrix = numerical_df.corr()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(20, 16))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap='RdBu_r', center=0,\n",
    "            square=True, linewidths=0.5, annot=False, fmt='.2f',\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTING_PATH / 'correlation_heatmap.png', dpi=150)\n",
    "plt.show()\n",
    "print(f\" Saved: correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify highly correlated pairs (|r| > 0.85)\n",
    "def find_high_correlations(corr_matrix, threshold=0.85):\n",
    "    \"\"\"Find feature pairs with correlation above threshold.\"\"\"\n",
    "    high_corr = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                high_corr.append({\n",
    "                    'Feature 1': corr_matrix.columns[i],\n",
    "                    'Feature 2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_matrix.iloc[i, j]\n",
    "                })\n",
    "    return pd.DataFrame(high_corr).sort_values('Correlation', ascending=False)\n",
    "\n",
    "high_corr_pairs = find_high_correlations(corr_matrix, threshold=0.85)\n",
    "\n",
    "print(\" MULTICOLLINEARITY DETECTION (|r| > 0.85)\")\n",
    "print(\"=\"*80)\n",
    "if len(high_corr_pairs) > 0:\n",
    "    print(f\"Found {len(high_corr_pairs)} highly correlated pairs:\")\n",
    "    display(high_corr_pairs)\n",
    "    print(\"\\n RECOMMENDATION: Consider dropping one feature from each pair.\")\n",
    "else:\n",
    "    print(\" No highly correlated feature pairs found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target (Churn)\n",
    "if target in numerical_df.columns:\n",
    "    target_corr = corr_matrix[target].drop(target).sort_values(key=abs, ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = ['#e74c3c' if x > 0 else '#2ecc71' for x in target_corr.head(20).values]\n",
    "    target_corr.head(20).plot(kind='barh', color=colors, edgecolor='black')\n",
    "    plt.title('Top 20 Features Correlated with Churn', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Correlation Coefficient')\n",
    "    plt.axvline(x=0, color='black', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTING_PATH / 'churn_correlations.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n TOP 10 CHURN PREDICTORS (by correlation):\")\n",
    "    for feat, corr in target_corr.head(10).items():\n",
    "        direction = \"↑ increases\" if corr > 0 else \"↓ decreases\"\n",
    "        print(f\"  {feat}: {corr:.4f} ({direction} churn)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MonthsInService Analysis (J-Curve Pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze churn by tenure (expecting J-curve pattern)\n",
    "if 'MonthsInService' in df.columns:\n",
    "    # Create tenure bins\n",
    "    df['TenureBin'] = pd.cut(df['MonthsInService'], \n",
    "                             bins=[0, 3, 6, 12, 24, 36, 100],\n",
    "                             labels=['0-3mo', '3-6mo', '6-12mo', '12-24mo', '24-36mo', '36+mo'])\n",
    "    \n",
    "    churn_by_tenure = df.groupby('TenureBin')[target].agg(['mean', 'count'])\n",
    "    churn_by_tenure.columns = ['Churn Rate', 'Count']\n",
    "    churn_by_tenure['Churn Rate'] = churn_by_tenure['Churn Rate'] * 100\n",
    "    \n",
    "    print(\" CHURN BY TENURE (Looking for J-curve)\")\n",
    "    display(churn_by_tenure)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    churn_by_tenure['Churn Rate'].plot(kind='bar', ax=ax, color='#3498db', edgecolor='black')\n",
    "    ax.axhline(y=df[target].mean()*100, color='red', linestyle='--', label='Overall Rate')\n",
    "    ax.set_title('Churn Rate by Customer Tenure', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Churn Rate (%)')\n",
    "    ax.set_xlabel('Tenure Segment')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTING_PATH / 'churn_by_tenure.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Drop temp column\n",
    "    df.drop('TenureBin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance (Preliminary Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare data for quick RF\n",
    "df_rf = df.copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "for col in df_rf.select_dtypes(include=['object']).columns:\n",
    "    df_rf[col] = df_rf[col].fillna('Unknown')\n",
    "    df_rf[col] = le.fit_transform(df_rf[col].astype(str))\n",
    "\n",
    "# Fill missing numerical values\n",
    "df_rf = df_rf.fillna(df_rf.median())\n",
    "\n",
    "# Exclude ID and leakage features\n",
    "exclude_cols = ['CustomerID', 'RetentionCalls', 'RetentionOffersAccepted', 'MadeCallToRetentionTeam', target]\n",
    "feature_cols = [c for c in df_rf.columns if c not in exclude_cols]\n",
    "\n",
    "X = df_rf[feature_cols]\n",
    "y = df_rf[target]\n",
    "\n",
    "print(f\"Training Quick RF on {X.shape[1]} features...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train quick Random Forest (non-tuned baseline)\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Visualize top 20\n",
    "plt.figure(figsize=(12, 8))\n",
    "importance_df.head(20).sort_values('Importance').plot(\n",
    "    kind='barh', x='Feature', y='Importance', \n",
    "    color='#9b59b6', edgecolor='black', legend=False\n",
    ")\n",
    "plt.title('Top 20 Features (Random Forest Importance)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTING_PATH / 'rf_feature_importance.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n TOP 15 FEATURES BY RF IMPORTANCE:\")\n",
    "for _, row in importance_df.head(15).iterrows():\n",
    "    print(f\"  {row['Feature']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key findings\n",
    "findings = {\n",
    "    'significant_ttest_features': ttest_df[ttest_df['P-Value'] < 0.05]['Feature'].tolist()[:20],\n",
    "    'significant_chi_features': chi_df[chi_df['P-Value'] < 0.05]['Feature'].tolist() if len(chi_df) > 0 else [],\n",
    "    'multicollinear_pairs': high_corr_pairs.to_dict('records') if len(high_corr_pairs) > 0 else [],\n",
    "    'top_rf_features': importance_df.head(20)['Feature'].tolist()\n",
    "}\n",
    "\n",
    "# Save to CSV\n",
    "ttest_df.to_csv(REPORTING_PATH / 'ttest_results.csv', index=False)\n",
    "if len(chi_df) > 0:\n",
    "    chi_df.to_csv(REPORTING_PATH / 'chi_square_results.csv', index=False)\n",
    "importance_df.to_csv(REPORTING_PATH / 'rf_importance.csv', index=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" EDA SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n T-Test significant features: {len(findings['significant_ttest_features'])}\")\n",
    "print(f\" Chi-Square significant features: {len(findings['significant_chi_features'])}\")\n",
    "print(f\" Multicollinear pairs (r>0.85): {len(findings['multicollinear_pairs'])}\")\n",
    "print(f\"\\n Top 5 Churn Predictors (RF):\")\n",
    "for feat in findings['top_rf_features'][:5]:\n",
    "    print(f\"   • {feat}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" NEXT: Proceed to 03_Preprocessing.ipynb\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
