{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Model Evaluation\n",
    "\n",
    "**Objective**: Comprehensive model evaluation on test set\n",
    "\n",
    "**Metrics**:\n",
    "- Confusion Matrix, Precision, Recall, F1\n",
    "- ROC-AUC, PR-AUC curves\n",
    "- Lift charts, threshold optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    roc_auc_score, average_precision_score,\n",
    "    roc_curve, precision_recall_curve,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print('Libraries loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "MODEL_PATH = Path('../data/06_models')\n",
    "REPORTING_PATH = Path('../data/08_reporting')\n",
    "REPORTING_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load models and data\n",
    "champion = joblib.load(MODEL_PATH / 'champion_model.pkl')\n",
    "lr_model = joblib.load(MODEL_PATH / 'logistic_regression.pkl')\n",
    "xgb_model = joblib.load(MODEL_PATH / 'xgboost.pkl')\n",
    "lgb_model = joblib.load(MODEL_PATH / 'lightgbm.pkl')\n",
    "scaler = joblib.load(MODEL_PATH / 'scaler.pkl')\n",
    "\n",
    "test_df = pd.read_csv(MODEL_PATH / 'test_set.csv')\n",
    "\n",
    "with open(MODEL_PATH / 'feature_list.json', 'r') as f:\n",
    "    FEATURES = json.load(f)\n",
    "\n",
    "TARGET = 'Churn'\n",
    "X_test = test_df[FEATURES]\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(f\"Churn rate: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale for LR\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Predictions\n",
    "models = {\n",
    "    'Logistic Regression': (lr_model, X_test_scaled),\n",
    "    'XGBoost': (xgb_model, X_test),\n",
    "    'LightGBM': (lgb_model, X_test)\n",
    "}\n",
    "\n",
    "predictions = {}\n",
    "for name, (model, X) in models.items():\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    predictions[name] = proba\n",
    "\n",
    "print(\" Predictions generated for all models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ROC-AUC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "for (name, proba), color in zip(predictions.items(), colors):\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    auc = roc_auc_score(y_test, proba)\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})', color=color, linewidth=2)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTING_PATH / 'roc_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for (name, proba), color in zip(predictions.items(), colors):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, proba)\n",
    "    ap = average_precision_score(y_test, proba)\n",
    "    ax.plot(recall, precision, label=f'{name} (AP={ap:.3f})', color=color, linewidth=2)\n",
    "\n",
    "# Baseline (random)\n",
    "baseline = y_test.mean()\n",
    "ax.axhline(y=baseline, color='gray', linestyle='--', label=f'Baseline ({baseline:.3f})')\n",
    "\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTING_PATH / 'pr_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Champion Model Detailed Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get champion predictions\n",
    "with open(MODEL_PATH / 'training_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "champion_name = metadata['champion_model']\n",
    "\n",
    "y_proba = predictions[champion_name]\n",
    "\n",
    "print(f\" CHAMPION MODEL: {champion_name}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold optimization\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "threshold_results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred = (y_proba >= thresh).astype(int)\n",
    "    threshold_results.append({\n",
    "        'Threshold': thresh,\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1': f1_score(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "thresh_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "# Find optimal threshold\n",
    "optimal_idx = thresh_df['F1'].idxmax()\n",
    "optimal_threshold = thresh_df.loc[optimal_idx, 'Threshold']\n",
    "\n",
    "print(f\" THRESHOLD ANALYSIS:\")\n",
    "display(thresh_df[thresh_df['Threshold'].isin([0.3, 0.4, 0.45, 0.5, optimal_threshold])])\n",
    "print(f\"\\n Optimal threshold (max F1): {optimal_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot threshold analysis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(thresh_df['Threshold'], thresh_df['Precision'], label='Precision', marker='o')\n",
    "ax.plot(thresh_df['Threshold'], thresh_df['Recall'], label='Recall', marker='s')\n",
    "ax.plot(thresh_df['Threshold'], thresh_df['F1'], label='F1', marker='^', linewidth=2)\n",
    "ax.axvline(x=optimal_threshold, color='red', linestyle='--', label=f'Optimal ({optimal_threshold:.2f})')\n",
    "ax.set_xlabel('Threshold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Precision-Recall Trade-off', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTING_PATH / 'threshold_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix at optimal threshold\n",
    "y_pred = (y_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Retained', 'Churned'],\n",
    "            yticklabels=['Retained', 'Churned'])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title(f'Confusion Matrix (threshold={optimal_threshold:.2f})', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTING_PATH / 'confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n CLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Retained', 'Churned']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lift Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lift(y_true, y_proba, n_bins=10):\n",
    "    \"\"\"Calculate lift by decile.\"\"\"\n",
    "    df = pd.DataFrame({'actual': y_true, 'proba': y_proba})\n",
    "    df['decile'] = pd.qcut(df['proba'], n_bins, labels=False, duplicates='drop')\n",
    "    \n",
    "    lift_df = df.groupby('decile').agg(\n",
    "        count=('actual', 'count'),\n",
    "        churners=('actual', 'sum'),\n",
    "        avg_proba=('proba', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    lift_df['churn_rate'] = lift_df['churners'] / lift_df['count'] * 100\n",
    "    baseline_rate = y_true.mean() * 100\n",
    "    lift_df['lift'] = lift_df['churn_rate'] / baseline_rate\n",
    "    \n",
    "    return lift_df.sort_values('decile', ascending=False)\n",
    "\n",
    "lift_df = calculate_lift(y_test.values, y_proba)\n",
    "\n",
    "print(\" LIFT BY DECILE:\")\n",
    "display(lift_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot lift chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "deciles = range(1, len(lift_df) + 1)\n",
    "ax.bar(deciles, lift_df['lift'].values, color='#3498db', edgecolor='black')\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', label='Baseline')\n",
    "\n",
    "ax.set_xlabel('Decile (1 = highest risk)')\n",
    "ax.set_ylabel('Lift')\n",
    "ax.set_title('Lift Chart by Decile', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(lift_df['lift'].values):\n",
    "    ax.text(i+1, v+0.1, f'{v:.2f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTING_PATH / 'lift_chart.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n TOP DECILE LIFT: {lift_df['lift'].values[0]:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Precision @ Top 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision at top 20%\n",
    "n_top20 = int(len(y_test) * 0.20)\n",
    "top20_idx = np.argsort(y_proba)[::-1][:n_top20]\n",
    "\n",
    "precision_top20 = y_test.iloc[top20_idx].mean()\n",
    "\n",
    "print(f\" PRECISION @ TOP 20%: {precision_top20*100:.2f}%\")\n",
    "print(f\"   (Baseline churn rate: {y_test.mean()*100:.2f}%)\")\n",
    "print(f\"   Improvement: {(precision_top20 / y_test.mean()):.2f}x over random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile final comparison\n",
    "comparison = []\n",
    "\n",
    "for name, proba in predictions.items():\n",
    "    pred = (proba >= 0.45).astype(int)\n",
    "    \n",
    "    comparison.append({\n",
    "        'Model': name,\n",
    "        'ROC-AUC': roc_auc_score(y_test, proba),\n",
    "        'PR-AUC': average_precision_score(y_test, proba),\n",
    "        'Precision': precision_score(y_test, pred),\n",
    "        'Recall': recall_score(y_test, pred),\n",
    "        'F1': f1_score(y_test, pred)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison).round(4)\n",
    "\n",
    "print(\" FINAL MODEL COMPARISON (Test Set):\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df)\n",
    "\n",
    "# Save\n",
    "comparison_df.to_csv(REPORTING_PATH / 'model_comparison.csv', index=False)\n",
    "print(f\"\\n Saved: model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n Champion: {champion_name}\")\n",
    "print(f\"   ROC-AUC: {comparison_df[comparison_df['Model']==champion_name]['ROC-AUC'].values[0]:.4f}\")\n",
    "print(f\"   PR-AUC:  {comparison_df[comparison_df['Model']==champion_name]['PR-AUC'].values[0]:.4f}\")\n",
    "print(f\"   Lift@Top10%: {lift_df['lift'].values[0]:.2f}x\")\n",
    "print(f\"   Precision@Top20%: {precision_top20*100:.2f}%\")\n",
    "print(\"\\n NEXT: Proceed to 08_Interpretation.ipynb\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}