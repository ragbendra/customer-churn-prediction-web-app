{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03 - Data Preprocessing\n",
                "\n",
                "**Objective**: Clean data and prepare for modeling\n",
                "\n",
                "**Steps**:\n",
                "1. Missing value imputation\n",
                "2. Outlier capping (99th percentile)\n",
                "3. Feature encoding (Label, One-Hot, Target)\n",
                "4. Save preprocessing pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "import joblib\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder\n",
                "from category_encoders import TargetEncoder\n",
                "\n",
                "print('Libraries loaded!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths\n",
                "RAW_PATH = Path('../data/01_raw')\n",
                "INTERMEDIATE_PATH = Path('../data/02_intermediate')\n",
                "PRIMARY_PATH = Path('../data/03_primary')\n",
                "MODEL_PATH = Path('../data/06_models')\n",
                "\n",
                "for p in [INTERMEDIATE_PATH, PRIMARY_PATH, MODEL_PATH]:\n",
                "    p.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Load data\n",
                "df_train = pd.read_csv(RAW_PATH / 'cell2celltrain.csv')\n",
                "df_holdout = pd.read_csv(RAW_PATH / 'cell2cellholdout.csv')\n",
                "\n",
                "print(f\"Train: {df_train.shape}\")\n",
                "print(f\"Holdout: {df_holdout.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define target and features to exclude\n",
                "TARGET = 'Churn'\n",
                "EXCLUDE_FEATURES = ['CustomerID', 'RetentionCalls', 'RetentionOffersAccepted', 'MadeCallToRetentionTeam']\n",
                "\n",
                "print(f\"Target: {TARGET}\")\n",
                "print(f\"Excluded (leakage): {EXCLUDE_FEATURES}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Missing Value Imputation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_missing(df):\n",
                "    \"\"\"Analyze missing values.\"\"\"\n",
                "    missing = df.isnull().sum()\n",
                "    missing_pct = (missing / len(df)) * 100\n",
                "    missing_df = pd.DataFrame({'Count': missing, 'Pct': missing_pct})\n",
                "    return missing_df[missing_df['Count'] > 0].sort_values('Pct', ascending=False)\n",
                "\n",
                "print(\"ðŸ“Š Missing Values (Before):\")\n",
                "display(analyze_missing(df_train))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def impute_missing(df, create_flags=True, flag_threshold=0.05):\n",
                "    \"\"\"Impute missing values with strategy based on % missing.\"\"\"\n",
                "    df = df.copy()\n",
                "    imputation_log = []\n",
                "    \n",
                "    for col in df.columns:\n",
                "        missing_pct = df[col].isnull().sum() / len(df)\n",
                "        \n",
                "        if missing_pct == 0:\n",
                "            continue\n",
                "            \n",
                "        # Create flag if > 5% missing\n",
                "        if create_flags and missing_pct > flag_threshold:\n",
                "            df[f'{col}_IsMissing'] = df[col].isnull().astype(int)\n",
                "            imputation_log.append({'Column': col, 'Action': f'Created {col}_IsMissing flag'})\n",
                "        \n",
                "        # Impute based on dtype\n",
                "        if df[col].dtype in ['int64', 'float64']:\n",
                "            # Numerical: median imputation\n",
                "            median_val = df[col].median()\n",
                "            df[col] = df[col].fillna(median_val)\n",
                "            imputation_log.append({'Column': col, 'Action': f'Median imputed: {median_val:.2f}'})\n",
                "        else:\n",
                "            # Categorical: 'Unknown' category\n",
                "            df[col] = df[col].fillna('Unknown')\n",
                "            imputation_log.append({'Column': col, 'Action': 'Filled with Unknown'})\n",
                "    \n",
                "    return df, pd.DataFrame(imputation_log)\n",
                "\n",
                "# Apply imputation\n",
                "df_train_imputed, imputation_log = impute_missing(df_train)\n",
                "df_holdout_imputed, _ = impute_missing(df_holdout, create_flags=True)\n",
                "\n",
                "print(\"ðŸ“Š Imputation Log:\")\n",
                "display(imputation_log)\n",
                "print(f\"\\nâœ… Missing values after: {df_train_imputed.isnull().sum().sum()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Outlier Capping (99th Percentile)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Features to cap\n",
                "CAP_FEATURES = ['MonthlyRevenue', 'MonthlyMinutes', 'OverageMinutes', 'RoamingCalls', 'TotalRecurringCharge']\n",
                "CAP_FEATURES = [f for f in CAP_FEATURES if f in df_train_imputed.columns]\n",
                "\n",
                "print(f\"Features to cap at 99th percentile: {CAP_FEATURES}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def cap_outliers(df, features, percentile=0.99):\n",
                "    \"\"\"Cap outliers at specified percentile.\"\"\"\n",
                "    df = df.copy()\n",
                "    cap_log = []\n",
                "    caps = {}\n",
                "    \n",
                "    for col in features:\n",
                "        if col not in df.columns:\n",
                "            continue\n",
                "            \n",
                "        cap_val = df[col].quantile(percentile)\n",
                "        n_capped = (df[col] > cap_val).sum()\n",
                "        \n",
                "        df[col] = df[col].clip(upper=cap_val)\n",
                "        \n",
                "        caps[col] = cap_val\n",
                "        cap_log.append({\n",
                "            'Feature': col,\n",
                "            'Cap Value': cap_val,\n",
                "            'Records Capped': n_capped,\n",
                "            'Pct Capped': (n_capped / len(df)) * 100\n",
                "        })\n",
                "    \n",
                "    return df, pd.DataFrame(cap_log), caps\n",
                "\n",
                "# Apply capping\n",
                "df_train_capped, cap_log, cap_values = cap_outliers(df_train_imputed, CAP_FEATURES)\n",
                "\n",
                "# Apply same caps to holdout\n",
                "df_holdout_capped = df_holdout_imputed.copy()\n",
                "for col, cap in cap_values.items():\n",
                "    if col in df_holdout_capped.columns:\n",
                "        df_holdout_capped[col] = df_holdout_capped[col].clip(upper=cap)\n",
                "\n",
                "print(\"ðŸ“Š Outlier Capping Log:\")\n",
                "display(cap_log)\n",
                "\n",
                "# Save cap values\n",
                "joblib.dump(cap_values, MODEL_PATH / 'outlier_caps.pkl')\n",
                "print(f\"\\nðŸ’¾ Saved: outlier_caps.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Feature Encoding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify feature types\n",
                "categorical_cols = df_train_capped.select_dtypes(include=['object']).columns.tolist()\n",
                "categorical_cols = [c for c in categorical_cols if c not in EXCLUDE_FEATURES]\n",
                "\n",
                "# Categorize by cardinality\n",
                "low_cardinality = []  # < 10 unique - One-Hot\n",
                "high_cardinality = []  # >= 10 unique - Target Encoding\n",
                "\n",
                "for col in categorical_cols:\n",
                "    n_unique = df_train_capped[col].nunique()\n",
                "    if n_unique < 10:\n",
                "        low_cardinality.append(col)\n",
                "    else:\n",
                "        high_cardinality.append(col)\n",
                "\n",
                "print(f\"Low cardinality (One-Hot): {low_cardinality}\")\n",
                "print(f\"High cardinality (Target Encoding): {high_cardinality}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# One-Hot Encoding for low cardinality\n",
                "df_train_encoded = df_train_capped.copy()\n",
                "df_holdout_encoded = df_holdout_capped.copy()\n",
                "\n",
                "if low_cardinality:\n",
                "    df_train_encoded = pd.get_dummies(df_train_encoded, columns=low_cardinality, drop_first=True)\n",
                "    df_holdout_encoded = pd.get_dummies(df_holdout_encoded, columns=low_cardinality, drop_first=True)\n",
                "    \n",
                "    # Align columns\n",
                "    missing_cols = set(df_train_encoded.columns) - set(df_holdout_encoded.columns)\n",
                "    for col in missing_cols:\n",
                "        df_holdout_encoded[col] = 0\n",
                "    df_holdout_encoded = df_holdout_encoded[df_train_encoded.columns.intersection(df_holdout_encoded.columns)]\n",
                "    \n",
                "    print(f\"âœ… One-Hot encoded: {low_cardinality}\")\n",
                "    print(f\"   New columns: {len(df_train_encoded.columns) - len(df_train_capped.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target Encoding for high cardinality (with fold-based to prevent leakage)\n",
                "if high_cardinality and TARGET in df_train_encoded.columns:\n",
                "    target_encoder = TargetEncoder(cols=high_cardinality, smoothing=1.0)\n",
                "    \n",
                "    # Fit on training data\n",
                "    df_train_encoded[high_cardinality] = target_encoder.fit_transform(\n",
                "        df_train_encoded[high_cardinality], \n",
                "        df_train_encoded[TARGET]\n",
                "    )\n",
                "    \n",
                "    # Transform holdout\n",
                "    for col in high_cardinality:\n",
                "        if col in df_holdout_encoded.columns:\n",
                "            df_holdout_encoded[col] = target_encoder.transform(df_holdout_encoded[[col]])\n",
                "    \n",
                "    # Save encoder\n",
                "    joblib.dump(target_encoder, MODEL_PATH / 'target_encoder.pkl')\n",
                "    print(f\"âœ… Target encoded: {high_cardinality}\")\n",
                "    print(f\"ðŸ’¾ Saved: target_encoder.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Remove Excluded Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Remove excluded features\n",
                "cols_to_drop = [c for c in EXCLUDE_FEATURES if c in df_train_encoded.columns]\n",
                "\n",
                "df_train_final = df_train_encoded.drop(columns=cols_to_drop, errors='ignore')\n",
                "df_holdout_final = df_holdout_encoded.drop(columns=cols_to_drop, errors='ignore')\n",
                "\n",
                "print(f\"Dropped columns: {cols_to_drop}\")\n",
                "print(f\"\\nFinal train shape: {df_train_final.shape}\")\n",
                "print(f\"Final holdout shape: {df_holdout_final.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Data Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validate preprocessing\n",
                "print(\"ðŸ“Š PREPROCESSING VALIDATION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Check for remaining missing values\n",
                "train_missing = df_train_final.isnull().sum().sum()\n",
                "print(f\"Missing values (train): {train_missing}\")\n",
                "\n",
                "# Check for infinite values\n",
                "numeric_cols = df_train_final.select_dtypes(include=['int64', 'float64']).columns\n",
                "inf_count = np.isinf(df_train_final[numeric_cols]).sum().sum()\n",
                "print(f\"Infinite values (train): {inf_count}\")\n",
                "\n",
                "# Check target distribution\n",
                "if TARGET in df_train_final.columns:\n",
                "    churn_rate = df_train_final[TARGET].mean() * 100\n",
                "    print(f\"Churn rate: {churn_rate:.2f}%\")\n",
                "\n",
                "# Data types summary\n",
                "print(f\"\\nData types:\")\n",
                "print(df_train_final.dtypes.value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Preprocessed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save preprocessed datasets\n",
                "df_train_final.to_csv(PRIMARY_PATH / 'preprocessed_train.csv', index=False)\n",
                "df_holdout_final.to_csv(PRIMARY_PATH / 'preprocessed_holdout.csv', index=False)\n",
                "\n",
                "# Save feature list\n",
                "feature_cols = [c for c in df_train_final.columns if c != TARGET]\n",
                "pd.DataFrame({'feature': feature_cols}).to_csv(PRIMARY_PATH / 'feature_list.csv', index=False)\n",
                "\n",
                "print(\"ðŸ’¾ Saved:\")\n",
                "print(f\"   - {PRIMARY_PATH / 'preprocessed_train.csv'}\")\n",
                "print(f\"   - {PRIMARY_PATH / 'preprocessed_holdout.csv'}\")\n",
                "print(f\"   - {PRIMARY_PATH / 'feature_list.csv'}\")\n",
                "\n",
                "print(f\"\\nâœ… Features: {len(feature_cols)}\")\n",
                "print(f\"âœ… Train samples: {len(df_train_final):,}\")\n",
                "print(f\"âœ… Holdout samples: {len(df_holdout_final):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ðŸ“‹ PREPROCESSING COMPLETE\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\nðŸ“Œ NEXT: Proceed to 04_Feature_Engineering.ipynb\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}